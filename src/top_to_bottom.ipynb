{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOm0etK4Y4H9",
        "outputId": "39ab5cf2-34f2-4ae4-a55b-f1607cf42066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from typing import Dict\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "\n",
        "# This will prompt for authorization.\n",
        "# drive.mount('/content/drive')\n",
        "# # Set the default path for data\n",
        "# data_path = '/content/drive/MyDrive/sentiment_analysis'\n",
        "# # Change the current working directory to the data path\n",
        "# os.chdir(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MvKoybrOd1Gp"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpaND8Q5fpie"
      },
      "source": [
        "## LSTM ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kRibBFwRa01e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix: np.ndarray):\n",
        "        \"\"\"\n",
        "        :param embedding_matrix: numpy array with vectors for all words\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        # number of words = number of rows in embedding matrix\n",
        "        num_words = embedding_matrix.shape[0]\n",
        "        # dimension of embedding is num of columns in the matrix\n",
        "        embed_dim = embedding_matrix.shape[1]\n",
        "\n",
        "        # we define an input embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_words, embedding_dim=embed_dim)\n",
        "\n",
        "        # embedding matrix is used as weights of the embedding layer\n",
        "        self.embedding.weight = nn.Parameter(\n",
        "            torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        # we don't want to train the pre-trained embeddings\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        # a simple bidirectional LSTM with hidden size of 128\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            128,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # output layer which is a linear layer, we have only one output\n",
        "        # input (512) = 128 + 128 for mean and same for max pooling\n",
        "        self.out = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass data through embedding layer\n",
        "        # the input is just the tokens\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # move embedding output to lstm\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        # apply mean and max pooling on lstm output\n",
        "        avg_pool = torch.mean(x, 1)\n",
        "        max_pool, _ = torch.max(x, 1)\n",
        "\n",
        "        # concatenate mean and max pooling\n",
        "        # this is why size is 512\n",
        "        # 128 for each direction = 256\n",
        "        # avg_pool = 256 and max_pool = 256\n",
        "        out = torch.cat((avg_pool, max_pool), 1)\n",
        "\n",
        "        # pass through the output layer and return the output\n",
        "        out = self.out(out)\n",
        "\n",
        "        # return linear output\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzj_zN-tfxiX"
      },
      "source": [
        "## Dataset ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-Iylsloyf05U"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset:\n",
        "    def __init__(self, reviews: np.array, targets: np.array):\n",
        "        \"\"\"\n",
        "        :param reviews: this is a numpy array\n",
        "        :param targets: a vector, numpy array\n",
        "        \"\"\"\n",
        "        self.reviews = reviews\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # returns length of the dataset\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, item) -> Dict[str, torch.tensor]:\n",
        "        # for any given item, which is an int,\n",
        "        # return review and targets as torch tensor\n",
        "        # item is the index of the item in concern\n",
        "        review = self.reviews[item, :]\n",
        "        target = self.targets[item]\n",
        "\n",
        "        return {\n",
        "            \"review\": torch.tensor(review, dtype=torch.long),\n",
        "            \"target\": torch.tensor(target, dtype=torch.float),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP94JdrQgANj"
      },
      "source": [
        "## train & evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u6-DFP2lgDCw"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    This is the main training function that trains model for one epoch\n",
        "    :param data_loader: this is the torch dataloader\n",
        "    :param model: model (lstm model)\n",
        "    :param optimizer: torch optimizer, e.g. adam, sgd, etc.\n",
        "    :param device: this can be \"cuda\" or \"cpu\"\n",
        "    \"\"\"\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # go through batches of data in data loader\n",
        "    for data in data_loader:\n",
        "        # fetch review and target from the dict\n",
        "        reviews = data[\"review\"]\n",
        "        targets = data[\"target\"]\n",
        "\n",
        "        # move the data to device that we want to use\n",
        "        reviews = reviews.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # make predictions from the model\n",
        "        predictions = model(reviews)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1, 1))\n",
        "\n",
        "        # compute gradient of loss w.r.t.\n",
        "        # all parameters of the model that are trainable\n",
        "        loss.backward()\n",
        "\n",
        "        # single optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(data_loader, model, device):\n",
        "    # initialize empty lists to store predictions and targets\n",
        "    final_predictions = []\n",
        "    final_targets = []\n",
        "\n",
        "    # put the model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            reviews = data[\"review\"]\n",
        "            targets = data[\"target\"]\n",
        "            reviews = reviews.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "            # make predictions\n",
        "            predictions = model(reviews)\n",
        "\n",
        "            # move predictions and targets to list\n",
        "            # we need to move predictions and targets to cpu too\n",
        "            predictions = predictions.cpu().numpy().tolist()\n",
        "            targets = data[\"target\"].cpu().numpy().tolist()\n",
        "            final_predictions.extend(predictions)\n",
        "            final_targets.extend(targets)\n",
        "\n",
        "    # return final predictions and targets\n",
        "    return final_predictions, final_targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW6zGyewgHM6"
      },
      "source": [
        "## Final ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MeJLNBn1gImx"
      },
      "outputs": [],
      "source": [
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(\" \")\n",
        "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_embedding_matrix(word_index, embedding_dict):\n",
        "    \"\"\"\n",
        "    This function creates the embedding matrix.\n",
        "    :param word_index: a dictionary with word:index_value\n",
        "    :param embedding_dict: a dictionary with word:embedding_vector\n",
        "    :return: a numpy array with embedding vectors for all known words\n",
        "    \"\"\"\n",
        "    # initialize matrix with zeros\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    # loop over all the words with tqdm progress bar\n",
        "    for word, i in tqdm(word_index.items(), desc=\"Creating Embedding Matrix\"):\n",
        "        # if word is found in pre-trained embeddings,\n",
        "        # update the matrix. if the word is not found,\n",
        "        # the vector is zeros!\n",
        "        if word in embedding_dict:\n",
        "            embedding_matrix[i] = embedding_dict[word]\n",
        "    # return embedding matrix\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "54O9JERwgJJa"
      },
      "outputs": [],
      "source": [
        "def run(df, fold):\n",
        "    \"\"\"\n",
        "    Run training and validation for a given fold\n",
        "    and dataset\n",
        "    :param df: pandas dataframe with kfold column\n",
        "    :param fold: current fold, int\n",
        "    \"\"\"\n",
        "    # fetch training dataframe\n",
        "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "    # fetch validation dataframe\n",
        "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    print(\"Fitting tokenizer\")\n",
        "    # we use tf.keras for tokenization\n",
        "    # you can use your own tokenizer and then you can\n",
        "    # get rid of tensorflow\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(df.review.values.tolist())\n",
        "\n",
        "    # convert training data to sequences\n",
        "    # for example : \"bad movie\" gets converted to\n",
        "    # [24, 27] where 24 is the index for bad and 27 is the\n",
        "    # index for movie\n",
        "    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
        "\n",
        "    # similarly convert validation data to\n",
        "    # sequences\n",
        "    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
        "\n",
        "    # zero pad the training sequences given the maximum length\n",
        "    # this padding is done on left hand side\n",
        "    # if sequence is > MAX_LEN, it is truncated on left hand side too\n",
        "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        xtrain, maxlen=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # zero pad the validation sequences\n",
        "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
        "\n",
        "    # initialize dataset class for training\n",
        "    train_dataset = IMDBDataset(\n",
        "        reviews=xtrain, targets=train_df.sentiment.values\n",
        "    )\n",
        "\n",
        "    # create torch dataloader for training\n",
        "    # torch dataloader loads the data using dataset\n",
        "    # class in batches specified by batch size\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2\n",
        "    )\n",
        "\n",
        "    # initialize dataset class for validation\n",
        "    valid_dataset = IMDBDataset(\n",
        "        reviews=xtest, targets=valid_df.sentiment.values\n",
        "    )\n",
        "\n",
        "    # create torch dataloader for validation\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=2\n",
        "    )\n",
        "\n",
        "    valid_dataset = IMDBDataset(\n",
        "        reviews=xtest, targets=valid_df.sentiment.values\n",
        "    )\n",
        "\n",
        "    # create torch dataloader for validation\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
        "    )\n",
        "    print(\"Loading embeddings\")\n",
        "\n",
        "    # load embeddings as shown previously\n",
        "    embedding_dict = load_vectors(\"crawl-300d-2M.vec\")\n",
        "    embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict)\n",
        "    # create torch device, since we use gpu, we are using cuda\n",
        "    device = torch.device(\"cuda\")\n",
        "    # fetch our LSTM model\n",
        "    model = LSTM(embedding_matrix)\n",
        "    # send model to device\n",
        "    model.to(device)\n",
        "\n",
        "    # initialize Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    print(\"Training Model\")\n",
        "    # set best accuracy to zero\n",
        "    best_accuracy = 0\n",
        "    # set early stopping counter to zero\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    # train and validate for all epochs\n",
        "    for epoch in range(EPOCHS):\n",
        "        # train one epoch\n",
        "        train(train_data_loader, model, optimizer, device)\n",
        "        # validate\n",
        "        outputs, targets = evaluate(valid_data_loader, model, device)\n",
        "        # use threshold of 0.5\n",
        "        # please note we are using linear layer and no sigmoid\n",
        "        # you should do this 0.5 threshold after sigmoid\n",
        "        outputs = np.array(outputs) >= 0.5\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(targets, outputs)\n",
        "        print(f\"FOLD:{fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\")\n",
        "        # simple early stopping\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        if early_stopping_counter > 2:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MazzXa-ogRt2",
        "outputId": "d22901c9-af33-4576-a85d-f4a5d4dc2784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting tokenizer\n",
            "Loading embeddings\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"imdb_folds.csv\")\n",
        "# train for all folds\n",
        "run(df, fold=0)\n",
        "run(df, fold=1)\n",
        "run(df, fold=2)\n",
        "run(df, fold=3)\n",
        "run(df, fold=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
